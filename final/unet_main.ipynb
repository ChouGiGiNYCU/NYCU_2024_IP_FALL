{"cells":[{"cell_type":"markdown","metadata":{"id":"YtttSHAggCOT"},"source":["#### import 需要用的 library"]},{"cell_type":"code","source":["pip install segmentation-models-pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nqNWda7HgQcO","executionInfo":{"status":"ok","timestamp":1735047091787,"user_tz":-480,"elapsed":11963,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}},"outputId":"dca2fc73-79d3-4063-b9cc-52fbcee7a9fb"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting segmentation-models-pytorch\n","  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\n","Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.27.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (11.0.0)\n","Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (1.17.0)\n","Collecting timm==0.9.7 (from segmentation-models-pytorch)\n","  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.20.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.5.1+cu121)\n","Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n","  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (24.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.12.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.0.2)\n","Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=6404958a4aca299f9443568fdee077941bfb36939c99a4ff777e08fae3495221\n","  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=9901dde9ac3431eee5d9521fd88c06a228d7689f63317eb225f0e6c9bbacaa9e\n","  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n","  Attempting uninstall: timm\n","    Found existing installation: timm 1.0.12\n","    Uninstalling timm-1.0.12:\n","      Successfully uninstalled timm-1.0.12\n","Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.4 timm-0.9.7\n"]}]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# os.chdir('/content/drive/My Drive/{你的google雲端硬碟資料夾位址}') #切換該目錄"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bBnWeeQkgkoH","executionInfo":{"status":"ok","timestamp":1735047114493,"user_tz":-480,"elapsed":22723,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}},"outputId":"6a2f34ab-cb7a-4b7b-cb73-408de4564426"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/IP/final') #切換該目錄"],"metadata":{"id":"yHnpkdRLhus9","executionInfo":{"status":"ok","timestamp":1735047114493,"user_tz":-480,"elapsed":10,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"IdYDF5TigCOV","executionInfo":{"status":"ok","timestamp":1735047136427,"user_tz":-480,"elapsed":15677,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}}},"outputs":[],"source":["from segmentation_models_pytorch import Unet\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as T\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms.functional as TF\n","import random\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import re"]},{"cell_type":"markdown","metadata":{"id":"btTEqoJdgCOX"},"source":["#### 初始化一些定義內容\n","\n","更改參數都在這裡"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"784fTMCIgCOX","executionInfo":{"status":"ok","timestamp":1735047169677,"user_tz":-480,"elapsed":485,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}}},"outputs":[],"source":["### init parameter\n","# Constants for UNet model training process\n","BATCH_SIZE = 2\n","IMG_HEIGHT = 512\n","IMG_WIDTH = 512\n","NUM_EPOCHS = 100\n","Learning_Ratio = 1e-3\n","THRESOLD = 0.5\n","\n","# Load data\n","test_img_dir = r'testing_dataset/image'\n","test_mask_dir = r'testing_dataset/mask'\n","train_img_dir = r'training_dataset/image'\n","train_mask_dir = r'training_dataset/mask'\n","# model_load_file = r'unet_25_test1_iou_0632_origin'"]},{"cell_type":"markdown","metadata":{"id":"MTGmGr2mgCOX"},"source":["#### 一些有用的function 寫在這裡\n"," 計算IOU..."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"8gws8y01gCOY","executionInfo":{"status":"ok","timestamp":1735047183297,"user_tz":-480,"elapsed":487,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}}},"outputs":[],"source":["def calculate_iou_tensor(image1, image2, threshold=THRESOLD):\n","    \"\"\"\n","    計算兩個 PyTorch Tensor 的交集比（IoU）。\n","\n","    :param image1: 第一張影像（PyTorch Tensor）\n","    :param image2: 第二張影像（PyTorch Tensor）\n","    :param threshold: 灰階二值化的閾值（預設 128）\n","    :return: IoU 值（介於 0 到 1）\n","    \"\"\"\n","    # 確保兩張影像的形狀相同\n","\n","    if image1.shape != image2.shape:\n","        raise ValueError(\"兩張影像必須具有相同的尺寸！\")\n","    # 將灰階影像二值化\n","    binary1 = (image1 >= threshold).to(torch.uint8)\n","    binary2 = (image2 >= threshold).to(torch.uint8)\n","\n","    # 計算交集和聯集\n","    intersection = torch.sum(binary1 & binary2).item()\n","    union = torch.sum(binary1 | binary2).item()\n","\n","    # 防止分母為零\n","    if union == 0:\n","        return 0.0\n","\n","    # 計算 IoU\n","    iou = intersection / union\n","    return iou"]},{"cell_type":"markdown","metadata":{"id":"muArQXM_gCOY"},"source":["##### Data Augmentation (資料增強)\n"," 做一些rotatation、hflip、vflip、Gaussian Blur、資料對比增強 等等"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Alc2PwgbgCOY","executionInfo":{"status":"ok","timestamp":1735047210171,"user_tz":-480,"elapsed":510,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}}},"outputs":[],"source":["def extract_number(filename):\n","    \"\"\"\n","    從檔名中提取第一個出現的數字並轉換為整數。\n","    如果找不到數字，則返回一個很大的數，使該檔案排在最後。\n","    \"\"\"\n","    name, _ = os.path.splitext(filename)\n","    match = re.search(r'\\d+', name)\n","    if match:\n","        return int(match.group())\n","    else:\n","        return float('inf')  # 沒有數字的檔案將排在最後\n","\n","\n","# 自定義變換類別\n","class ImageRotateFlipTransform:\n","    def __call__(self, img, mask):\n","        # 同步水平翻轉\n","        if random.random() > 0.5:\n","            img = T.functional.hflip(img)\n","            mask = T.functional.hflip(mask)\n","        if random.random() > 0.5:\n","            img = T.functional.vflip(img)\n","            mask = T.functional.vflip(mask)\n","        # 同步旋轉\n","        angles = [0,90,270]\n","        # angles = [0]\n","        angle = random.choice(angles)  # 隨機選擇旋轉角度\n","        img = T.functional.rotate(img, angle)\n","        mask = T.functional.rotate(mask, angle)\n","\n","        return img, mask\n","\n","class customDataSet(Dataset):\n","  def __init__(self, img_dir, mask_dir, transform_img,transform_mask,Rotate_Flag=True):\n","    super().__init__()\n","    self.img_dir = img_dir\n","    self.mask_dir = mask_dir\n","    self.transform_img = transform_img\n","    self.transform_mask = transform_mask\n","    self.images = [f for f in os.listdir(img_dir)] #僅讀取副檔名為以下的\n","    self.images = sorted(self.images, key=extract_number)\n","    self.masks  = [f for f in os.listdir(mask_dir)]\n","    self.masks  = sorted(self.masks, key=extract_number)\n","    self.rotate_flag =  Rotate_Flag\n","    self.ImageRotateFlipTransform = ImageRotateFlipTransform()\n","  def __len__(self):\n","    return len(self.images)\n","\n","  def __getitem__(self, idx):\n","    img_path = os.path.join(self.img_dir, self.images[idx])\n","    mask_path = os.path.join(self.mask_dir, self.masks[idx])\n","    image = Image.open(img_path).convert('RGB')\n","    mask = Image.open(mask_path).convert('L') # 轉成黑白圖片 0 或 255\n","\n","    if self.rotate_flag :\n","        image , mask = self.ImageRotateFlipTransform(image,mask)\n","    if self.transform_img and self.transform_mask:\n","       image = self.transform_img(image)\n","       mask  = self.transform_mask(mask)\n","\n","\n","    return image,mask"]},{"cell_type":"markdown","metadata":{"id":"OJblQTPRgCOZ"},"source":["#### setting DataLoader\n","\n","loading moel 使用Unet 並且 backbone 採用 resenet34 ， 不使用 pre-train 參數  \n","\n","設定train 裡面要做的 data augmentation 的一些方法，並且resize成一樣的大小"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"27ee7WCLgCOZ","executionInfo":{"status":"ok","timestamp":1735047231142,"user_tz":-480,"elapsed":3154,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}},"outputId":"0b6f2d99-2dfa-41ae-a9cf-83205b0319a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Use the GPU to train\n"]}],"source":["\n","# model_load_file = None\n","# 使用 ResNet34 作為 UNet 的 backbone\n","model = Unet(encoder_name=\"resnet34\",\n","             encoder_weights=None,  # 不用pre-train的權重\n","             in_channels=3,\n","             classes=1,\n","             )\n","\n","# Check the device we are using is GPU or CPU\n","if torch.cuda.is_available():\n","  device = torch.device('cuda')\n","  print('Use the GPU to train')\n","else:\n","  device = torch.device('cpu')\n","  print('Use the CPU to train')\n","model = model.to(device)\n","\n","transform_img = T.Compose([\n","                T.ToTensor(),                                  # 轉換為 Tensor\n","                # T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","                T.Resize((IMG_HEIGHT, IMG_WIDTH)),            # 調整圖片大小\n","                T.RandomApply([\n","                    T.GaussianBlur(kernel_size=7)\n","                ], p=1),                                   # 以 50% 機率進行模糊\n","                T.RandomApply([\n","                    T.ColorJitter(\n","                        brightness=0.4,                           # 隨機調整亮度 (0.8 ~ 1.2 範圍)\n","                        contrast=0.5,                             # 隨機調整對比度\n","                        saturation=0.2,                           # 飽和度不變 (設為 0)\n","                        hue=0.1                                   # 色調不變\n","                    )             # 隨機高斯模糊 (核大小設為 5)\n","                ],p=1)\n","            ])\n","\n","# valid 都不做resize 直接算原始的 iou 大小\n","transform_test_img = T.Compose([T.ToTensor(),\n","                  # T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","                  T.Resize((IMG_HEIGHT, IMG_WIDTH)),\n","                  T.GaussianBlur(kernel_size=3)\n","                  ])\n","transform_mask  = T.Compose([T.ToTensor(),\n","                T.Resize((IMG_HEIGHT, IMG_WIDTH)),\n","                ])\n","train_data    = customDataSet(train_img_dir, train_mask_dir,transform_img,transform_mask,Rotate_Flag=True)\n","valid_dataset = customDataSet(test_img_dir, test_mask_dir ,transform_test_img,transform_mask,Rotate_Flag=False)\n","\n","train_loader  = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,pin_memory=True)\n","valid_loader  = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False,pin_memory=True)\n","\n","loss_function = nn.BCEWithLogitsLoss() # 內部會幫忙做sigmoid\n","optimizer = optim.Adam(model.parameters(),lr=Learning_Ratio) # Choosing Adam as our optimizer\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5) # new_lr = lr*factor"]},{"cell_type":"markdown","metadata":{"id":"StfuuTd1gCOa"},"source":["#### save、load model function"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"4c6c0KOygCOa","executionInfo":{"status":"ok","timestamp":1735047269565,"user_tz":-480,"elapsed":473,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}}},"outputs":[],"source":["def save_model(model, optimizer, epoch, best_iou, save_path):\n","    \"\"\"\n","    保存模型的狀態字典和優化器的狀態字典。\n","    :param model: 要保存的模型\n","    :param optimizer: 優化器\n","    :param epoch: 當前 epoch\n","    :param best_iou: 當前最佳的 IoU 分數\n","    :param save_path: 模型保存的路徑\n","    \"\"\"\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'best_iou': best_iou\n","    }\n","    torch.save(checkpoint, save_path)\n","    print(f\"模型已保存到 {save_path}\")\n"]},{"cell_type":"markdown","metadata":{"id":"-a9-jVc7gCOa"},"source":["### 定義train function"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"9XZI1FATgCOa","executionInfo":{"status":"ok","timestamp":1735047280251,"user_tz":-480,"elapsed":475,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}}},"outputs":[],"source":["def train(model, num_epochs, train_loader,valid_loader, optimizer,loss_function,device):\n","    model.to(device)\n","    best_iou = 0\n","    best_acc = 0\n","    for epoch in range(num_epochs):\n","        num_correct = 0\n","        num_pixels = 0\n","        total_loss = 0\n","        ################## train model\n","        model.train()\n","        for count, (x, y) in enumerate(train_loader):\n","            x = x.to(device) # [BATCH_SIZE , CHANNEL , WIDTH , HEIGHT]\n","            y = y.to(device) # [BATCH_SIZE , CHANNEL , WIDTH , HEIGHT]\n","            out = model(x) # out range [-inf , inf]\n","            ############ ACC #######\n","            # out = torch.sigmoid(out) # loss 要是BCELoss\n","            loss = loss_function(out, y)\n","            total_loss += loss.item()\n","            predictions = (torch.sigmoid(out) >= THRESOLD) # 預測出來的mask樣子\n","            origin_mask = (y>=THRESOLD)     # 原本train_set 裡面的 mask\n","            num_correct += (predictions==origin_mask).sum().item()\n","            # num_pixels += BATCH_SIZE*IMG_WIDTH*IMG_HEIGHT\n","            num_pixels += BATCH_SIZE*origin_mask.size(2)*origin_mask.size(3)\n","            ##############################\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","        acc_mean = num_correct/num_pixels\n","        avg_loss = total_loss / len(train_loader)\n","        if best_acc <= acc_mean:\n","           best_acc = acc_mean\n","           save_model(model, optimizer, epoch, best_iou, f\"unet_{epoch+1}_test1_acc\")\n","        print(f'Epoch[{epoch+1}] Acc: {acc_mean:.3f} - AvgLoss: {avg_loss:.3f}')\n","        ################# eval model\n","        model.eval()\n","        with torch.no_grad():\n","            iou_total = 0\n","            total_number_picture = 0\n","            valid_loss = 0\n","            for count, (x, y) in enumerate(valid_loader):\n","                batch_size = x.size(0)\n","                total_number_picture += batch_size\n","                x = x.to(device)\n","                y = y.to(device)\n","                out  = model(x)\n","                ###### 計算驗證損失\n","                loss = loss_function(out, y)\n","                valid_loss += loss.item()\n","                ###############################\n","                out = torch.sigmoid(out) # loss 要是BCELoss\n","                for i in range(batch_size):\n","                    # 分別取出第 i 張影像及對應的預測與標註遮罩\n","                    # 注意確保維度匹配你的 iou_function 輸入格式\n","                    img_single = x[i].squeeze(0)    # [C, H, W] -> [1, C, H, W]\n","                    mask_single = y[i].squeeze(0)   # [H, W] -> [1, H, W] 或 [1, 1, H, W]\n","\n","                    predic_mask = out[i].squeeze(0) # [C, H, W] -> [1, C, H, W]\n","\n","                    # 利用你的 iou 函式計算該張影像的 IoU\n","                    iou_total += calculate_iou_tensor(mask_single,predic_mask)\n","\n","        avg_valid_loss = valid_loss / len(valid_loader)\n","        scheduler.step(avg_valid_loss)  # 使用驗證損失來調整學習率\n","\n","        iou_mean = (iou_total/total_number_picture)\n","        if best_iou <= iou_mean:\n","            best_iou = iou_mean\n","            save_model(model, optimizer, epoch, best_iou, f\"unet_{epoch+1}_test1_iou\")\n","        print(f'Epoch[{epoch+1}] - Test | IOU_Mean : {iou_mean:.3f} | AvgLoss : {avg_valid_loss:.3f} ')\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":478},"id":"nbZpURB3gCOa","executionInfo":{"status":"error","timestamp":1735047459517,"user_tz":-480,"elapsed":171899,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}},"outputId":"1505c7f0-b9af-4329-e9de-78a80300edb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["模型已保存到 unet_1_test1_acc\n","Epoch[1] Acc: 0.606 - AvgLoss: 0.642\n","模型已保存到 unet_1_test1_iou\n","Epoch[1] - Test | IOU_Mean : 0.326 | AvgLoss : 0.673 \n","模型已保存到 unet_2_test1_acc\n","Epoch[2] Acc: 0.634 - AvgLoss: 0.633\n","模型已保存到 unet_2_test1_iou\n","Epoch[2] - Test | IOU_Mean : 0.385 | AvgLoss : 0.789 \n","Epoch[3] Acc: 0.572 - AvgLoss: 0.666\n","Epoch[3] - Test | IOU_Mean : 0.366 | AvgLoss : 0.645 \n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-34cf53237b0d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# start to train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-1044d5c2d124>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, train_loader, valid_loader, optimizer, loss_function, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m################## train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [BATCH_SIZE , CHANNEL , WIDTH , HEIGHT]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [BATCH_SIZE , CHANNEL , WIDTH , HEIGHT]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-f175a280b50c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mmask_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 轉成黑白圖片 0 或 255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# start to train\n","train(model, NUM_EPOCHS, train_loader,valid_loader,optimizer,loss_function,device)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}