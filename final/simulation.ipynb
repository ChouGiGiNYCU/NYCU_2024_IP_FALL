{"cells":[{"cell_type":"code","source":["pip install segmentation-models-pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-QapUh_BjN1h","executionInfo":{"status":"ok","timestamp":1735047548639,"user_tz":-480,"elapsed":21798,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}},"outputId":"4a76762c-ca73-4b6a-f1a7-a76135b3e871"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting segmentation-models-pytorch\n","  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\n","Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.27.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (11.0.0)\n","Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (1.17.0)\n","Collecting timm==0.9.7 (from segmentation-models-pytorch)\n","  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.20.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.5.1+cu121)\n","Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n","  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (24.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.12.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.0.2)\n","Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=51bcf525a96f49e2e98af727f298f0ccc0d7d19b7df2a9719df044e099b1be89\n","  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=246b5ab13d8338dda0194720e938c986d8edabd9b9e0c017c4ecd5ecc6587d75\n","  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n","  Attempting uninstall: timm\n","    Found existing installation: timm 1.0.12\n","    Uninstalling timm-1.0.12:\n","      Successfully uninstalled timm-1.0.12\n","Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.4 timm-0.9.7\n"]}]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/IP/final') #切換該目錄"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-jPCoELyjRkW","executionInfo":{"status":"ok","timestamp":1735047571710,"user_tz":-480,"elapsed":23086,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}},"outputId":"52e5c9d9-cd3e-4146-b2ca-f767a8fa6484"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Pzaf1k3kgDhW","executionInfo":{"status":"ok","timestamp":1735047584351,"user_tz":-480,"elapsed":12652,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}}},"outputs":[],"source":["from segmentation_models_pytorch import Unet\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import numpy as np\n","import torchvision.transforms.functional as TF\n","import random\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import re"]},{"cell_type":"markdown","metadata":{"id":"dWYjsjCZgDhX"},"source":["#### 初始化一些參數\n","\n","輸入正確的檔案path 即可 run"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"AHCFJF2SgDhY","executionInfo":{"status":"ok","timestamp":1735047586065,"user_tz":-480,"elapsed":8,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}}},"outputs":[],"source":["### init parameter\n","# Constants for UNet model training process\n","BATCH_SIZE = 4\n","IMG_HEIGHT = 512\n","IMG_WIDTH = 512\n","NUM_EPOCHS = 200\n","Learning_Ratio = 2e-3\n","THRESOLD = 0.5\n","\n","# Load data (please change)\n","test_img_dir = r'testing_dataset/image'\n","test_mask_dir = r'testing_dataset/mask'\n","test_mask_output_dir = r'testing_dataset/output'\n","train_img_dir = r'training_dataset/image'\n","train_mask_dir = r'training_dataset/mask'\n","train_mask_output_dir = r'training_dataset/output'\n","model_load_file = r'unet_25_test1_iou_0632_origin'"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"xMOG9IMogDhZ","executionInfo":{"status":"ok","timestamp":1735047618952,"user_tz":-480,"elapsed":926,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}}},"outputs":[],"source":["def calculate_iou_tensor(image1, image2, threshold=THRESOLD):\n","    \"\"\"\n","    計算兩個 PyTorch Tensor 的交集比（IoU）。\n","\n","    :param image1: 第一張影像（PyTorch Tensor）\n","    :param image2: 第二張影像（PyTorch Tensor）\n","    :param threshold: 灰階二值化的閾值（預設 128）\n","    :return: IoU 值（介於 0 到 1）\n","    \"\"\"\n","    # 確保兩張影像的形狀相同\n","\n","    if image1.shape != image2.shape:\n","        raise ValueError(\"兩張影像必須具有相同的尺寸！\")\n","    # 將灰階影像二值化\n","    binary1 = (image1 >= threshold).to(torch.uint8)\n","    binary2 = (image2 >= threshold).to(torch.uint8)\n","\n","    # 計算交集和聯集\n","    intersection = torch.sum(binary1 & binary2).item()\n","    union = torch.sum(binary1 | binary2).item()\n","\n","    # 防止分母為零\n","    if union == 0:\n","        return 0.0\n","\n","    # 計算 IoU\n","    iou = intersection / union\n","    return iou"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"oy578lX1gDhZ","executionInfo":{"status":"ok","timestamp":1735047624186,"user_tz":-480,"elapsed":569,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}}},"outputs":[],"source":["def extract_number(filename):\n","    \"\"\"\n","    從檔名中提取第一個出現的數字並轉換為整數。\n","    如果找不到數字，則返回一個很大的數，使該檔案排在最後。\n","    \"\"\"\n","    name, _ = os.path.splitext(filename)\n","    match = re.search(r'\\d+', name)\n","    if match:\n","        return int(match.group())\n","    else:\n","        return float('inf')  # 沒有數字的檔案將排在最後\n","\n","class customDataSet(Dataset):\n","  def __init__(self, img_dir, mask_dir, transform_img,transform_mask):\n","    super().__init__()\n","    self.img_dir = img_dir\n","    self.mask_dir = mask_dir\n","    self.transform_img = transform_img\n","    self.transform_mask = transform_mask\n","    self.images = [f for f in os.listdir(img_dir)] #僅讀取副檔名為以下的\n","    self.images = sorted(self.images, key=extract_number)\n","    self.masks  = [f for f in os.listdir(mask_dir)]\n","    self.masks  = sorted(self.masks, key=extract_number)\n","  def __len__(self):\n","    return len(self.images)\n","\n","  def __getitem__(self, idx):\n","    img_path = os.path.join(self.img_dir, self.images[idx])\n","    mask_path = os.path.join(self.mask_dir, self.masks[idx])\n","    image = Image.open(img_path).convert('RGB')\n","    mask = Image.open(mask_path).convert('L') # 轉成黑白圖片 0 或 255\n","\n","    if self.transform_img and self.transform_mask:\n","       image = self.transform_img(image)\n","       mask  = self.transform_mask(mask)\n","\n","\n","    return image,mask"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eMDqcJi8gDha","executionInfo":{"status":"ok","timestamp":1735047633829,"user_tz":-480,"elapsed":4538,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}},"outputId":"ecea1428-7279-4bc2-a9b5-cd6a7d220f84"},"outputs":[{"output_type":"stream","name":"stdout","text":["Use the GPU to train\n"]}],"source":["model = Unet(encoder_name=\"resnet34\",\n","             encoder_weights=None,  # 不用pre-train的權重\n","             in_channels=3,\n","             classes=1,\n","             )\n","\n","# Check the device we are using is GPU or CPU\n","if torch.cuda.is_available():\n","  device = torch.device('cuda')\n","  print('Use the GPU to train')\n","else:\n","  device = torch.device('cpu')\n","  print('Use the CPU to train')\n","model = model.to(device)\n","\n","transform_img = T.Compose([\n","                T.ToTensor(),                                  # 轉換為 Tensor\n","                # T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","                T.Resize((IMG_HEIGHT, IMG_WIDTH))            # 調整圖片大小\n","            ])\n","\n","# valid 都不做resize 直接算原始的 iou 大小\n","transform_test_img = T.Compose([T.ToTensor(),\n","                                # T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","                                T.Resize((IMG_HEIGHT, IMG_WIDTH)),\n","                                T.GaussianBlur(kernel_size=3)\n","                                ])\n","transform_mask  = T.Compose([T.ToTensor(),\n","                                T.Resize((IMG_HEIGHT, IMG_WIDTH)),\n","                            ])\n","train_data    = customDataSet(train_img_dir, train_mask_dir,transform_img,transform_mask)\n","valid_dataset = customDataSet(test_img_dir, test_mask_dir ,transform_test_img,transform_mask)\n","\n","train_loader  = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False,pin_memory=True)\n","valid_loader  = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False,pin_memory=True)\n","loss_function = nn.BCEWithLogitsLoss() # 內部會幫忙做sigmoid\n","optimizer = optim.Adam(model.parameters(),lr=Learning_Ratio) # Choosing Adam as our optimizer\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5) # new_lr = lr*factor"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"_3-VNHXbgDhc","executionInfo":{"status":"ok","timestamp":1735047633830,"user_tz":-480,"elapsed":4,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}}},"outputs":[],"source":["def load_model(model, load_path,device):\n","    \"\"\"\n","    加載已保存的模型和優化器狀態。\n","\n","    :param model: 模型實例\n","    :param optimizer: 優化器實例\n","    :param load_path: 模型保存的路徑\n","    :param device: 設備（CPU 或 GPU）\n","    :return: 加載後的模型、優化器、epoch 和 best_iou\n","    \"\"\"\n","    checkpoint = torch.load(load_path, map_location=device)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    best_iou = checkpoint['best_iou']\n","\n","    print(f\"已加載模型於 epoch {epoch+1}，最佳 IoU: {best_iou:.4f}\")\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"Vca0H53xgDhd"},"source":["#### Loading model"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKURG-dQgDhe","executionInfo":{"status":"ok","timestamp":1735047652560,"user_tz":-480,"elapsed":5403,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}},"outputId":"079e7405-a250-468f-9e5a-48a979d48e57"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-7a1daaef822f>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(load_path, map_location=device)\n"]},{"output_type":"stream","name":"stdout","text":["已加載模型於 epoch 25，最佳 IoU: 0.6322\n"]}],"source":["if model_load_file is not None:\n","   model = load_model(model,model_load_file,device)"]},{"cell_type":"markdown","metadata":{"id":"jm8Yqmy1gDhf"},"source":["### show 出圖片 function"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Mud9OH4AgDhf","executionInfo":{"status":"ok","timestamp":1735047667643,"user_tz":-480,"elapsed":454,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","\n","def show_image_and_mask(img, mask, origin):\n","    \"\"\"\n","    顯示圖像及其對應的遮罩。\n","\n","    :param img: 圖像 (torch.Tensor)\n","    :param mask: 遮罩 (torch.Tensor)\n","    \"\"\"\n","    # 確保圖像是 NumPy 陣列，並將圖像轉換為 HWC 格式\n","    img = img.permute(1, 2, 0).cpu().detach().numpy()  # HWC 格式\n","\n","    # 如果圖像是單通道，則轉換為灰度圖\n","    if img.shape[2] == 1:\n","        img = img.squeeze(-1)  # 去掉單通道維度 (變成 2D)\n","\n","    # 確保 mask 是 NumPy 陣列\n","    mask = mask.cpu().detach().numpy()\n","    origin = origin.cpu().detach().numpy()\n","    # 如果 mask 是 3D，則選擇單通道\n","    if mask.ndim == 3:\n","        mask = mask.squeeze(0)  # 去掉最後一維 (如果是單通道)\n","    if origin.ndim == 3:\n","        origin = origin.squeeze(0)  # 去掉最後一維 (如果是單通道)\n","    # 創建畫布並顯示圖像與遮罩\n","    fig, ax = plt.subplots(1, 3, figsize=(12, 6))\n","\n","    # 顯示圖像\n","    ax[0].imshow(img)\n","    ax[0].set_title(\"Image\")\n","    ax[0].axis('off')\n","\n","    # 顯示遮罩\n","    ax[1].imshow(mask, cmap='gray')  # 使用灰度色圖顯示遮罩\n","    ax[1].set_title(\"Predict\")\n","    ax[1].axis('off')\n","\n","    ax[2].imshow(origin, cmap='gray')  # 使用灰度色圖顯示遮罩\n","    ax[2].set_title(\"Origin\")\n","    ax[2].axis('off')\n","    # 顯示圖像\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"38z8-J4wgDhf"},"source":["#### 計算train 、test 的 IOU 平均 (512X512)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1miFrvAhlzTNg5N-BOYa700YVb7DNgidt"},"id":"5OtspDZsgDhf","executionInfo":{"status":"ok","timestamp":1735047745620,"user_tz":-480,"elapsed":40195,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}},"outputId":"20c6db7c-2910-45cc-f702-4137c9bbbae4"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["model.eval()\n","iou_mean = 0\n","out_path = test_mask_output_dir\n","# model.eval()\n","total_img = 0\n","with torch.no_grad():\n","      for count, (x, y) in enumerate(valid_loader,start=0):\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","\n","            predict_img = model(x)\n","            batch_size = x.size(0)\n","            for i in range(batch_size):\n","\n","                  total_img += 1\n","                  input_img = x[i].squeeze(0)\n","\n","                  origin_y = y[i]>=THRESOLD\n","                  origin_y = origin_y.squeeze(0)\n","\n","\n","                  probability = torch.sigmoid(predict_img[i])>=THRESOLD\n","                  predict_mask = probability.squeeze(0)\n","                  ###### save img\n","                   # 保存 predict_mask 512x512 寫進output\n","                  predict_mask_np = predict_mask.cpu().numpy().astype(np.uint8) * 255  # 转换为 0 和 255\n","                  mask_image = Image.fromarray(predict_mask_np)\n","                  mask_filename = os.path.join(out_path, f\"{count*BATCH_SIZE+(i+1)}.png\")\n","                  mask_image.save(mask_filename)\n","                  ###########\n","                  iou = calculate_iou_tensor(origin_y,predict_mask)\n","                  iou_mean += iou\n","                  # print(f\"{count} : input_img : {input_img.size()} | predict_mask : {predict_mask.size()} | origin_mask : {origin_y.size()}\")\n","                  # print(f\"IOU : {iou}\")\n","                  show_image_and_mask(input_img,predict_mask,origin_y)\n","\n","      print(f\"IOU_MEAN : {iou_mean/total_img}\")\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1u7LG3DHatTvKLPJBz7Z4WjG4BYHbDrGg"},"id":"YP51YsfegDhg","executionInfo":{"status":"ok","timestamp":1735047912012,"user_tz":-480,"elapsed":138344,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}},"outputId":"6f2c70d7-262f-4f38-9c5b-221f98913a06"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["model.eval()\n","iou_mean = 0\n","out_dir = train_mask_output_dir\n","total_img_num=0\n","with torch.no_grad():\n","      for count, (x, y) in enumerate(train_loader,start=0):\n","            x = x.to(device)\n","            y = y.to(device)\n","            predict_img = model(x)\n","            batch_size = x.size(0)\n","            for i in range(batch_size):\n","                  total_img_num+=1\n","                  input_img = x[i].squeeze(0)\n","\n","                  origin_y = y[i]>=THRESOLD\n","                  origin_y = origin_y.squeeze(0)\n","\n","\n","                  probability = torch.sigmoid(predict_img[i])>=THRESOLD\n","                  predict_mask = probability.squeeze(0)\n","                  ###### save img\n","                   # 保存 predict_mask\n","                  predict_mask_np = predict_mask.cpu().numpy().astype(np.uint8) * 255  # 转换为 0 和 255\n","                  mask_image = Image.fromarray(predict_mask_np)\n","                  mask_filename = os.path.join(out_dir, f\"{count*BATCH_SIZE+(i+1)}.png\")\n","                  mask_image.save(mask_filename)\n","                  ###########\n","                  iou = calculate_iou_tensor(origin_y,predict_mask)\n","                  iou_mean += iou\n","                  # print(f\"input_img : {input_img.size()} | predict_mask : {predict_mask.size()} | origin_mask : {origin_y.size()}\")\n","                  # print(f\"IOU : {iou}\")\n","                  show_image_and_mask(input_img,predict_mask,origin_y)\n","\n","      print(f\"IOU_MEAN : {iou_mean/total_img_num}\")"]},{"cell_type":"markdown","metadata":{"id":"8HiUynnygDhg"},"source":["#### 讀取資料夾裡面的output(512x512) 和對應的原圖做原始的 IOU\n","###### 會先對output 做Resize 到 一樣的大小，並且存起來。\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6uCoNa_agDhg","executionInfo":{"status":"ok","timestamp":1735047961591,"user_tz":-480,"elapsed":2562,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}},"outputId":"bd2c2cb8-0680-4a97-a54e-069a8911a745"},"outputs":[{"output_type":"stream","name":"stdout","text":["TRAIN_Data - IOU_MEAN : 0.6502\n"]}],"source":["import os\n","from PIL import Image\n","from torchvision import transforms as T\n","from torchvision.transforms import InterpolationMode\n","# path\n","train_origin = train_mask_dir\n","train_output = train_mask_output_dir\n","\n","# get origin file name\n","origin = sorted([os.path.join(train_origin, f) for f in os.listdir(train_origin) ])\n","output = sorted([os.path.join(train_output, f) for f in os.listdir(train_output) ])\n","\n","# cehck img size is same\n","if len(origin) != len(output):\n","    print(f\"The number of origin : {len(origin)} and output files:{len(output)} does not match!\")\n","    exit()\n","\n","# transform tensor\n","transform_origin = T.Compose([\n","    T.ToTensor()\n","])\n","\n","# 初始化 IoU\n","iou = 0\n","\n","# count IoU\n","for origin_path, output_path in zip(origin, output):\n","    # img file open\n","    train_origin_mask = Image.open(origin_path).convert('L')  # 轉成黑白圖像\n","    train_output_mask = Image.open(output_path).convert('L')  # 轉成黑白圖像\n","\n","    # transform tensor\n","    train_origin_mask_tensor = transform_origin(train_origin_mask) >= 0.3  # 轉成 0、1\n","\n","    height = train_origin_mask_tensor.shape[1]\n","    width  = train_origin_mask_tensor.shape[2]\n","\n","    transform_output = T.Compose([\n","                        T.ToTensor(),\n","                        T.Resize((height,width))\n","                    ])\n","    train_output_mask_tensor = transform_output(train_output_mask) >= 0.3  # 轉成 0、1\n","    ##################################\n","    predict_mask = train_output_mask_tensor.squeeze(0) #[1xheightxwidth] -> [heightxwidth]\n","    predict_mask_np = predict_mask.cpu().numpy().astype(np.uint8) * 255  # 轉成 0、25\n","    mask_image = Image.fromarray(predict_mask_np)\n","\n","\n","    mask_filename = f\"{output_path}\"\n","    mask_image.save(mask_filename)\n","    ####################################\n","    # count IoU\n","    iou += calculate_iou_tensor(train_origin_mask_tensor, train_output_mask_tensor)\n","\n","# print IoU\n","print(f'TRAIN_Data - IOU_MEAN : {iou / len(origin):.4f}')\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YmM8VBmSgDhh","executionInfo":{"status":"ok","timestamp":1735048022837,"user_tz":-480,"elapsed":930,"user":{"displayName":"GiGi Chou","userId":"16983208759404772214"}},"outputId":"9b62a248-a247-46c9-d400-259bd8d4b64c"},"outputs":[{"output_type":"stream","name":"stdout","text":["TEST_Data - IOU_MEAN : 0.6209\n"]}],"source":["import os\n","from PIL import Image\n","from torchvision import transforms as T\n","\n","# file path\n","test_origin = test_mask_dir\n","test_output = test_mask_output_dir\n","\n","# get all file name\n","origin = sorted([os.path.join(test_origin, f) for f in os.listdir(test_origin) ])\n","output = sorted([os.path.join(test_output, f) for f in os.listdir(test_output) ])\n","\n","# check size is same\n","if len(origin) != len(output):\n","    print(f\"The number of origin : {len(origin)} and output files:{len(output)} does not match!\")\n","    exit()\n","\n","# transform tensor\n","transform_origin = T.Compose([\n","    T.ToTensor()\n","])\n","\n","# init IoU\n","iou = 0\n","\n","# count IoU\n","for origin_path, output_path in zip(origin, output):\n","    #open the mask file\n","    test_origin_mask = Image.open(origin_path).convert('L')  # 轉成黑白圖像\n","    test_output_mask = Image.open(output_path).convert('L')  # 轉成黑白圖像\n","\n","    # transform to tensor\n","    test_origin_mask_tensor = transform_origin(test_origin_mask) >= 0.5  # 轉成 0、1\n","\n","    height = test_origin_mask_tensor.shape[1]\n","    width  = test_origin_mask_tensor.shape[2]\n","\n","    transform_output = T.Compose([\n","                            T.ToTensor(),\n","                            T.Resize((height,width))\n","                        ])\n","    test_output_mask_tensor = transform_output(test_output_mask) >= 0.5  # 轉成 0、1\n","    ##################################\n","    predict_mask = test_output_mask_tensor.squeeze(0) #[1xheightxwidth] -> [heightxwidth]\n","    predict_mask_np = predict_mask.cpu().numpy().astype(np.uint8) * 255  # 轉成 0、255\n","    # print(predict_mask_np.shape)\n","    mask_image = Image.fromarray(predict_mask_np)\n","    mask_filename = f\"{output_path}\"\n","    mask_image.save(mask_filename)\n","    ####################################\n","    # count IoU\n","    iou += calculate_iou_tensor(test_origin_mask_tensor, test_output_mask_tensor)\n","\n","# print IoU\n","print(f'TEST_Data - IOU_MEAN : {iou / len(origin):.4f}')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}